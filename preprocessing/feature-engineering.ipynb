{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "X_train = pickle.load(open('../dataset/processed-data/X_train.pkl', 'rb'))\n",
    "y_train = pickle.load(open('../dataset/processed-data/y_train.pkl', 'rb'))\n",
    "\n",
    "X_test = pickle.load(open('../dataset/processed-data/X_test.pkl', 'rb'))\n",
    "y_test = pickle.load(open('../dataset/processed-data/y_test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4736 2030\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loạt hoạt_động trải nghiệm tại tuần_lễ du_lịch bà rịa vũng_tàu tuâ lê du li ch vơ nhiều hoạt_động như lễ_hội diều hội_chợ ẩm_thực trình_diễn nghệ_thuật diễn ra tư nga dự_kiến thu_hút hàng nghìn du_khách với chủ_đề let go ba ria vung tau tuần_lễ du_lịch nhằm thu_hút khách du_lịch trong nước và quốc_tế góp_phần phát_triển kinh_tế xã_hội của tỉnh thời_điểm mùa thu thời_tiết mát_mẻ dễ_chịu nhất trong năm việc tổ_chức sự_kiện làm cho du_khách nhớ đến một bà rịa vũng_tàu hấp_dẫn quanh_năm bà trần thị thu hiền phó_giám_đốc sở du_lịch chia_sẻ tại họp_báo hôm chuỗi hoạt_động của sự_kiện được thiết_kế đa_dạng liên_tục tại khu_vực công_viên san_hô xanh biển đông bãi sau thành_phố vũng_tàu và khu_vực biển_hồ tràm huyện xuyên mộc với nghĩa mỗi ngày là một trải nghiệm mới_lạ cho du_khách trong đó các hoạt_động nổi_bật là lễ_hội diều lướt_ván diều triển_lãm các tiểu_cảnh và mô_hình_tượng cát hội_chợ ẩm_thực và bia hội_chợ ocop one commune one product mỗi xã một sản_phẩm giới_thiệu nhiều đặc_sản của địa_phương hội_thảo về phát_triển du_lịch xanh cũng trong tuần_lễ này sự_kiện du_lịch lần đầu_tiên được tổ_chức tại bà rịa vũng_tàu là giải golf giao_hữu giải chạy đêm vung tau discovery night run và trải nghiệm dù lượn trong chương_trình nghệ_thuật khai_mạc tuần_lễ du_lịch vào tối có trình_diễn drone show với sự tham_gia của các dj và nhiều ca_sĩ nổi_tiếng dịp này tỉnh còn đăng_cai tổ_chức bán_kết cuộc thi hoa_hậu đại_sứ du_lịch từ đến ngày tại bãi sau ban tổ_chức cho biết sẽ kiểm_soát an_ninh trật_tự an_toàn vệ_sinh thực_phẩm môi_trường để du_khách và người dân an_tâm vui_chơi ông phạm ngọc hải chủ_tịch hiệp_hội du_lịch tỉnh cho biết sẽ vận_động doanh_nghiệp hội_viên xây_dựng nhiều gói combo dịch_vụ cho từng nhóm phân khúc khách với chính_sách giá ưu_đãi kết_hợp tri ân du_khách trong dịp này ban tổ_chức kỳ_vọng sự_kiện giúp địa_phương phá băng mùa du_lịch thấp_điểm kim_ánh'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'du-lich'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Count Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a count vectorizer object\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(X_train)\n",
    "\n",
    "# Transform the training and testing data using count vectorizer object\n",
    "X_train_count = count_vect.transform(X_train)\n",
    "X_test_count = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Tf-Idf Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word level - we choose max number of words equal to 30000 except all words (100k+ words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "\n",
    "# Assume that we don't have test set before\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram level - we choose max number of words equal to 30000 except all words (100k+ words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', max_features=30000, ngram_range=(2, 3))\n",
    "tfidf_vect_ngram.fit(X_train)\n",
    "X_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "\n",
    "# Assume that we don't have test set before\n",
    "X_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram-char level - we choose max number of words equal to 30000 except all words (100k+ words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_ngram_char = TfidfVectorizer(analyzer='char', max_features=30000, ngram_range=(2, 3))\n",
    "tfidf_vect_ngram_char.fit(X_train)\n",
    "X_train_tfidf_ngram_char =  tfidf_vect_ngram_char.transform(X_train)\n",
    "\n",
    "# Assume that we don't have test set before\n",
    "X_test_tfidf_ngram_char =  tfidf_vect_ngram_char.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Singular value decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd.fit(X_train_tfidf)\n",
    "\n",
    "X_train_tfidf_svd = svd.transform(X_train_tfidf)\n",
    "X_test_tfidf_svd = svd.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_ngram = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd_ngram.fit(X_train_tfidf_ngram)\n",
    "\n",
    "X_train_tfidf_ngram_svd = svd_ngram.transform(X_train_tfidf_ngram)\n",
    "X_test_tfidf_ngram_svd = svd_ngram.transform(X_test_tfidf_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Char level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_ngram_char = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd_ngram_char.fit(X_train_tfidf_ngram_char)\n",
    "\n",
    "X_train_tfidf_ngram_char_svd = svd_ngram_char.transform(X_train_tfidf_ngram_char)\n",
    "X_test_tfidf_ngram_char_svd = svd_ngram_char.transform(X_test_tfidf_ngram_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "word2vec_model_path = os.path.join(dir_path, \"dataset/vi/wordvectors/data/vi.bin\")\n",
    "\n",
    "w2v = KeyedVectors.load(word2vec_model_path)\n",
    "vocab = list(w2v.wv.index_to_key)\n",
    "wv = w2v.wv\n",
    "\n",
    "def get_word2vec_data(X):\n",
    "    word2vec_data = []\n",
    "    for x in X:\n",
    "        sentence = []\n",
    "        for word in x.split(\" \"):\n",
    "            if word in vocab:\n",
    "                sentence.append(wv[word])\n",
    "        word2vec_data.append(sentence)\n",
    "    return(word2vec_data)\n",
    "\n",
    "X_train_w2v = get_word2vec_data(X_train)\n",
    "X_test_w2v = get_word2vec_data(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VNC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
